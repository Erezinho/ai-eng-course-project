{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 1: Imports and configuration loaded.\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, random, pathlib, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import faiss\n",
    "import gradio as gr\n",
    "\n",
    "# --- Main Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_DIR = Path(\"./data\")\n",
    "CACHE_DIR = Path(\"./cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Tuning Knobs ---\n",
    "CLASSES_TO_USE = None\n",
    "IMAGES_PER_CLASS = 60\n",
    "TOP_K = 6\n",
    "BATCH_SIZE = 64\n",
    "EMB_CACHE_NAME = \"food101_clip_ViT-B-32_subset\"\n",
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "# --- Reproducibility ---\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "print(\"✅ Cell 1: Imports and configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 2: Food-101 dataset loaded. Found 75750 images.\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_ds = datasets.Food101(root=str(DATA_DIR), download=True, split='train')\n",
    "\n",
    "class_to_idx = train_ds.class_to_idx\n",
    "idx_to_class = {v:k for k,v in class_to_idx.items()}\n",
    "\n",
    "print(f\"✅ Cell 2: Food-101 dataset loaded. Found {len(train_ds)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Create or Load Cached Image Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loading pre-computed subset from cache: cache/subset_data.pkl\n",
      "✅ Cell 3: Subset ready with 6060 images.\n"
     ]
    }
   ],
   "source": [
    "# Define the path for our cache file\n",
    "subset_cache_path = CACHE_DIR / \"subset_data.pkl\"\n",
    "\n",
    "# Caching logic: Check if the pre-computed subset file exists\n",
    "if subset_cache_path.exists():\n",
    "    print(f\"✅ Loading pre-computed subset from cache: {subset_cache_path}\")\n",
    "    with open(subset_cache_path, 'rb') as f:\n",
    "        cached_data = pickle.load(f)\n",
    "    subset_indices = cached_data['subset_indices']\n",
    "    \n",
    "else:\n",
    "    print(\"Cache not found. Building subset from scratch. This may take a minute...\")\n",
    "    if CLASSES_TO_USE is not None:\n",
    "        keep_idx = {class_to_idx[c] for c in CLASSES_TO_USE if c in class_to_idx}\n",
    "    else:\n",
    "        keep_idx = set(range(len(class_to_idx)))\n",
    "\n",
    "    all_labels = [label for _, label in tqdm(train_ds, desc=\"Extracting labels\")]\n",
    "\n",
    "    per_class_counter = {i:0 for i in keep_idx}\n",
    "    subset_indices = []\n",
    "    for idx, label in enumerate(all_labels):\n",
    "        if label in keep_idx and per_class_counter[label] < IMAGES_PER_CLASS:\n",
    "            subset_indices.append(idx)\n",
    "            per_class_counter[label] += 1\n",
    "        if all(per_class_counter[i] >= IMAGES_PER_CLASS for i in keep_idx):\n",
    "            break\n",
    "            \n",
    "    print(f\"✅ Subset created. Caching result to {subset_cache_path}\")\n",
    "    with open(subset_cache_path, 'wb') as f:\n",
    "        pickle.dump({'subset_indices': subset_indices}, f)\n",
    "\n",
    "print(f\"✅ Cell 3: Subset ready with {len(subset_indices)} images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Load CLIP Model & Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model and processor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 4: CLIP model loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading CLIP model and processor...\")\n",
    "model = CLIPModel.from_pretrained(MODEL_NAME, use_safetensors=True).to(DEVICE).eval()\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "print(\"✅ Cell 4: CLIP model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Build or Load Image Embedding Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached embeddings & index...\n",
      "✅ Cell 5: Index ready. Contains 6060 vectors.\n"
     ]
    }
   ],
   "source": [
    "emb_cache_base = CACHE_DIR / EMB_CACHE_NAME\n",
    "img_emb_path = emb_cache_base.with_suffix(\".npy\")\n",
    "meta_path    = emb_cache_base.with_suffix(\".meta.json\")\n",
    "faiss_path   = emb_cache_base.with_suffix(\".faiss\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = [b[0] for b in batch]\n",
    "    labels = [b[1] for b in batch]\n",
    "    return images, labels\n",
    "\n",
    "def compute_image_embeddings():\n",
    "    subset_for_loader = torch.utils.data.Subset(train_ds, subset_indices)\n",
    "    loader = DataLoader(subset_for_loader, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    \n",
    "    all_embs = []\n",
    "    all_meta = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Embedding images\"):\n",
    "            # This is the critical fix for the recurring error.\n",
    "            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "            \n",
    "            image_emb = model.get_image_features(**inputs)\n",
    "            image_emb /= image_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "            all_embs.append(image_emb.detach().cpu().numpy())\n",
    "            for lab in labels:\n",
    "                all_meta.append({\"label_idx\": int(lab), \"label\": idx_to_class[int(lab)]})\n",
    "    return np.vstack(all_embs).astype(\"float32\"), all_meta\n",
    "\n",
    "if img_emb_path.exists() and meta_path.exists() and faiss_path.exists():\n",
    "    print(\"Loading cached embeddings & index...\")\n",
    "    img_embs = np.load(img_emb_path)\n",
    "    with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "    index = faiss.read_index(str(faiss_path))\n",
    "else:\n",
    "    print(\"No cache found. Computing embeddings from scratch...\")\n",
    "    img_embs, meta = compute_image_embeddings()\n",
    "    np.save(img_emb_path, img_embs)\n",
    "    with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "    dim = img_embs.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(img_embs)\n",
    "    faiss.write_index(index, str(faiss_path))\n",
    "\n",
    "print(f\"✅ Cell 5: Index ready. Contains {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Map Subset Indices to File Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cell 6: Corrected 6060 file paths for the subset.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "if not hasattr(train_ds, \"_image_files\"):\n",
    "    raise AttributeError(\"Your version of torchvision's Food101 is missing the `_image_files` attribute.\")\n",
    "\n",
    "# FINAL FIX: The `_image_files` list already contains the full relative path.\n",
    "# We will use these paths directly without prepending any root directory.\n",
    "all_image_paths = [Path(p) for p in train_ds._image_files]\n",
    "\n",
    "# Select only the paths for the images that are in our created subset\n",
    "subset_image_paths = [all_image_paths[i] for i in subset_indices]\n",
    "\n",
    "print(f\"✅ Cell 6: Corrected {len(subset_image_paths)} file paths for the subset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Define Search Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUGGING FILE PATHS ---\n",
      "Path for rank #1: data/food-101/images/apple_pie/1111062.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #2: data/food-101/images/pizza/1202925.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #3: data/food-101/images/pizza/1029698.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #4: data/food-101/images/pizza/1097980.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #5: data/food-101/images/pizza/1054420.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #6: data/food-101/images/caesar_salad/1167058.jpg\n",
      "Does file exist? -> True\n",
      "--------------------------\n",
      "\n",
      "✅ Cell 7: Search functions defined. Test search completed.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def text_to_embedding(query: str) -> np.ndarray:\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(text=[query], return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        text_emb = model.get_text_features(**inputs)\n",
    "        text_emb /= text_emb.norm(p=2, dim=-1, keepdim=True)\n",
    "    return text_emb.detach().cpu().numpy().astype(\"float32\")\n",
    "\n",
    "def search_images(query: str, k: int = TOP_K):\n",
    "    if not query.strip(): return []\n",
    "    \n",
    "    q_emb = text_to_embedding(query)\n",
    "    sims, idxs = index.search(q_emb, k)\n",
    "    \n",
    "    # --- Start of Debugging Block ---\n",
    "    print(\"\\n--- DEBUGGING FILE PATHS ---\")\n",
    "    results = []\n",
    "    for rank, (i, s) in enumerate(zip(idxs[0], sims[0]), start=1):\n",
    "        img_path = subset_image_paths[i]\n",
    "        label = meta[i][\"label\"]\n",
    "        caption = f\"#{rank}: {label.replace('_',' ')} (Score: {s:.3f})\"\n",
    "        \n",
    "        # We'll print the path and check if the file exists\n",
    "        print(f\"Path for rank #{rank}: {img_path}\")\n",
    "        print(f\"Does file exist? -> {img_path.exists()}\")\n",
    "        \n",
    "        results.append((str(img_path), caption))\n",
    "    print(\"--------------------------\\n\")\n",
    "    # --- End of Debugging Block ---\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Quick smoke test ---\n",
    "test_results = search_images(\"pizza with cheese\")\n",
    "print(f\"✅ Cell 7: Search functions defined. Test search completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Launch the Gradio User Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DEBUGGING FILE PATHS ---\n",
      "Path for rank #1: data/food-101/images/chocolate_cake/1077967.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #2: data/food-101/images/chocolate_cake/1169806.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #3: data/food-101/images/chocolate_cake/1001084.jpg\n",
      "Does file exist? -> True\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- DEBUGGING FILE PATHS ---\n",
      "Path for rank #1: data/food-101/images/filet_mignon/11785.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #2: data/food-101/images/filet_mignon/1236710.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #3: data/food-101/images/filet_mignon/1001477.jpg\n",
      "Does file exist? -> True\n",
      "--------------------------\n",
      "\n",
      "\n",
      "--- DEBUGGING FILE PATHS ---\n",
      "Path for rank #1: data/food-101/images/filet_mignon/1007877.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #2: data/food-101/images/steak/114601.jpg\n",
      "Does file exist? -> True\n",
      "Path for rank #3: data/food-101/images/filet_mignon/11785.jpg\n",
      "Does file exist? -> True\n",
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "EXAMPLES = [\n",
    "    \"pizza margherita\",\n",
    "    \"chocolate cake\",\n",
    "    \"sushi roll with salmon\",\n",
    "    \"chicken curry\",\n",
    "    \"greek salad\",\n",
    "    \"ramen noodles\",\n",
    "    \"hamburger with fries\",\n",
    "]\n",
    "\n",
    "with gr.Blocks(title=\"Food Dish Image Search\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"## 🍔 Food Dish Image Search 🍣\")\n",
    "    gr.Markdown(\"Enter a dish name (e.g., 'spaghetti bolognese') and see the closest matching images from the Food-101 dataset.\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=2):\n",
    "            query = gr.Textbox(label=\"Dish Description\", placeholder=\"e.g., 'spaghetti bolognese'\")\n",
    "            k = gr.Slider(1, 12, value=TOP_K, step=1, label=\"Number of Images to Find\")\n",
    "            btn = gr.Button(\"Search\", variant=\"primary\")\n",
    "        with gr.Column(scale=3):\n",
    "             # Simplified for better compatibility\n",
    "             gallery = gr.Gallery(label=\"Results\", columns=3)\n",
    "    \n",
    "    btn.click(search_images, inputs=[query, k], outputs=gallery)\n",
    "    gr.Examples(EXAMPLES, inputs=[query])\n",
    "\n",
    "# CORRECTED LINE: Set share=True to create a public link.\n",
    "#demo.launch(share=True)\n",
    "# FINAL FIX: Use inline=True to embed the app directly in the notebook output.\n",
    "demo.launch(inline=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
